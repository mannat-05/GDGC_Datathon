{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef761663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v4, Final) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Engineering features...\n",
      "Converting data types...\n",
      "Training on 587201 samples, validating on 146801 samples.\n",
      "Applying preprocessing...\n",
      "Preprocessing complete. Final training data types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 587201 entries, 391931 to 121958\n",
      "Data columns (total 35 columns):\n",
      " #   Column                           Non-Null Count   Dtype   \n",
      "---  ------                           --------------   -----   \n",
      " 0   Len_Circuit_inkm                 587201 non-null  float64 \n",
      " 1   Laps                             587201 non-null  float64 \n",
      " 2   Start_Position                   587201 non-null  float64 \n",
      " 3   Formula_Avg_Speed_kmh            587201 non-null  float64 \n",
      " 4   Humidity_%                       587201 non-null  float64 \n",
      " 5   Champ_Points                     587201 non-null  float64 \n",
      " 6   Champ_Position                   587201 non-null  float64 \n",
      " 7   race_year                        587201 non-null  float64 \n",
      " 8   seq                              587201 non-null  float64 \n",
      " 9   position                         587201 non-null  float64 \n",
      " 10  points                           587201 non-null  float64 \n",
      " 11  Corners_in_Lap                   587201 non-null  float64 \n",
      " 12  Tire_Degradation_Factor_per_Lap  587201 non-null  float64 \n",
      " 13  Pit_Stop_Duration_Seconds        587201 non-null  float64 \n",
      " 14  Ambient_Temperature_Celsius      587201 non-null  float64 \n",
      " 15  Track_Temperature_Celsius        587201 non-null  float64 \n",
      " 16  air                              587201 non-null  float64 \n",
      " 17  ground                           587201 non-null  float64 \n",
      " 18  starts                           587201 non-null  float64 \n",
      " 19  finishes                         587201 non-null  float64 \n",
      " 20  with_points                      587201 non-null  float64 \n",
      " 21  podiums                          587201 non-null  float64 \n",
      " 22  wins                             587201 non-null  float64 \n",
      " 23  is_raining                       587201 non-null  float64 \n",
      " 24  temp_diff                        587201 non-null  float64 \n",
      " 25  Rider_ID                         587201 non-null  category\n",
      " 26  Formula_category_x               587201 non-null  category\n",
      " 27  Formula_Track_Condition          587201 non-null  category\n",
      " 28  Tire_Compound                    587201 non-null  category\n",
      " 29  Penalty                          488825 non-null  category\n",
      " 30  Session                          587201 non-null  category\n",
      " 31  Formula_shortname                587201 non-null  category\n",
      " 32  circuit_name                     587201 non-null  category\n",
      " 33  weather                          587201 non-null  category\n",
      " 34  track                            587201 non-null  category\n",
      "dtypes: category(10), float64(25)\n",
      "memory usage: 123.0 MB\n",
      "None\n",
      "Defining model to run on CPU (GPU not found)...\n",
      "\n",
      "ðŸŽï¸  Starting model training...\n",
      "âœ… Training complete in 91.16 seconds.\n",
      "\n",
      "--- ðŸ Model Evaluation ---\n",
      "Validation RMSE: 11.2226 seconds\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Engineering features...\n",
      "Making predictions on test data...\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[01:59:48] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\../data/cat_container.h:28: Invalid new DataFrame input for the: 25th feature (0-based). The data type doesn't match the one used in the training dataset. Both should be either numeric or categorical. For a categorical feature, the index type must match between the training and test set.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 212\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# --- 13. Make Final Predictions ---\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking predictions on test data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test_final\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# --- 14. Create Submission File ---\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\sklearn.py:1443\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1443\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1451\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[0;32m   1452\u001b[0m             cp \u001b[38;5;241m=\u001b[39m import_cupy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:774\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    773\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:2887\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (ArrowTransformed, PandasTransformed)):\n\u001b[1;32m-> 2887\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2888\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterPredictFromColumnar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2889\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2891\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2896\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _prediction_output(shape, dims, preds, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\xgboost\\core.py:323\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [01:59:48] C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\data\\../data/cat_container.h:28: Invalid new DataFrame input for the: 25th feature (0-based). The data type doesn't match the one used in the training dataset. Both should be either numeric or categorical. For a categorical feature, the index type must match between the training and test set."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "print(\"--- Starting F1 Lap Time Model Training (v4, Final) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "\n",
    "# --- 2. Load Your Training Data (Memory Efficient) ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    # Set dtypes for categories AND numeric columns that might be objects\n",
    "    # This is a safety measure\n",
    "    dtype_map = {col: 'category' for col in CATEGORICAL_FEATURES}\n",
    "    for col in NUMERIC_FEATURES:\n",
    "        dtype_map[col] = 'float64' # Load numeric columns as floats\n",
    "        \n",
    "    df_raw = pd.read_csv('train.csv', dtype=dtype_map)\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    exit()\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: Could not load all dtypes automatically: {e}\")\n",
    "    print(\"Loading with default dtypes and will convert later.\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "\n",
    "\n",
    "# --- 3. Feature Engineering Function ---\n",
    "def engineer_features(df):\n",
    "    \"\"\"Creates new features.\"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in NUMERIC_FEATURES:\n",
    "            NUMERIC_FEATURES.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns and 'Ambient_Temperature_Celsius' in df.columns:\n",
    "        # Ensure columns are numeric before subtraction\n",
    "        df['Track_Temperature_Celsius'] = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        df['Ambient_Temperature_Celsius'] = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = df['Track_Temperature_Celsius'] - df['Ambient_Temperature_Celsius']\n",
    "        if 'temp_diff' not in NUMERIC_FEATURES:\n",
    "            NUMERIC_FEATURES.append('temp_diff')\n",
    "            \n",
    "    return df\n",
    "\n",
    "# --- 4. Apply Feature Engineering ---\n",
    "df = engineer_features(df_raw.copy())\n",
    "\n",
    "# --- Data Type Conversion (Critical) ---\n",
    "# Ensure all numeric features are actually numeric (float)\n",
    "# and categorical features are 'category'\n",
    "print(\"Converting data types...\")\n",
    "for col in NUMERIC_FEATURES:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce') # Force to numeric\n",
    "        \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Safety checks\n",
    "NUMERIC_FEATURES = [col for col in NUMERIC_FEATURES if col in df.columns]\n",
    "CATEGORICAL_FEATURES = [col for col in CATEGORICAL_FEATURES if col in df.columns]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    print(f\"FATAL ERROR: Target column '{TARGET}' not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- 5. Create Data Splits ---\n",
    "X = df.drop(TARGET, axis=1)\n",
    "y = df[TARGET]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- 6. Set up Preprocessing (FIXED) ---\n",
    "# This pipeline will *only* process numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create the ColumnTransformer to apply the numeric pipeline\n",
    "# We set remainder='drop' because we will handle categorical columns manually\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, NUMERIC_FEATURES)\n",
    "    ],\n",
    "    remainder='drop' # <-- This is the key change\n",
    ")\n",
    "\n",
    "# --- 7. Apply Preprocessing Manually (FIXED) ---\n",
    "print(\"Applying preprocessing...\")\n",
    "\n",
    "# Fit the preprocessor on the training data's numeric columns\n",
    "preprocessor.fit(X_train[NUMERIC_FEATURES])\n",
    "\n",
    "# --- New function to apply transform and rebuild DataFrame ---\n",
    "def preprocess_data(df, processor):\n",
    "    # 1. Transform the numeric features\n",
    "    scaled_numeric_data = processor.transform(df[NUMERIC_FEATURES])\n",
    "    \n",
    "    # 2. Put the scaled data back into a DataFrame\n",
    "    df_numeric_scaled = pd.DataFrame(scaled_numeric_data, columns=NUMERIC_FEATURES, index=df.index)\n",
    "    \n",
    "    # 3. Get the original categorical features (and ensure correct type)\n",
    "    df_categorical = df[CATEGORICAL_FEATURES].astype('category')\n",
    "    \n",
    "    # 4. Concatenate the scaled numeric and original categorical features\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "# Apply the new preprocessing to our train and validation sets\n",
    "X_train_final = preprocess_data(X_train, preprocessor)\n",
    "X_val_final = preprocess_data(X_val, preprocessor)\n",
    "\n",
    "print(\"Preprocessing complete. Final training data types:\")\n",
    "print(X_train_final.info())\n",
    "\n",
    "# --- 8. Define the XGBoost Model (GPU Enabled) ---\n",
    "# --- 8. Define the XGBoost Model (CPU Fallback) ---\n",
    "print(\"Defining model to run on CPU (GPU not found)...\")\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    \n",
    "    # --- THIS IS THE FIX ---\n",
    "    tree_method='hist',      # Use the fast CPU method\n",
    "    # ----------------------\n",
    "    \n",
    "    enable_categorical=True      # Natively handle categories\n",
    ")\n",
    "\n",
    "# --- 9. Train the Model (No Pipeline) ---\n",
    "print(\"\\nðŸŽï¸  Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model directly on the preprocessed DataFrames\n",
    "model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_final, y_val)], \n",
    "    verbose=False \n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Training complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 10. Evaluate Your New Model ---\n",
    "y_pred = model.predict(X_val_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "print(f\"\\n--- ðŸ Model Evaluation ---\")\n",
    "print(f\"Validation RMSE: {rmse:.4f} seconds\")\n",
    "\n",
    "# --- 11. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "try:\n",
    "    df_test_raw = pd.read_csv('test.csv')\n",
    "    submission_ids = df_test_raw['id']\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'test.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# --- 12. Apply the SAME Transformations to Test Data ---\n",
    "df_test_engineered = engineer_features(df_test_raw.copy())\n",
    "\n",
    "# Ensure dtypes are correct *before* preprocessing\n",
    "for col in NUMERIC_FEATURES:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = df_test_engineered[col].astype('category')\n",
    "        \n",
    "df_test_final = preprocess_data(df_test_engineered, preprocessor)\n",
    "\n",
    "# --- 13. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = model.predict(df_test_final)\n",
    "\n",
    "# --- 14. Create Submission File ---\n",
    "print(\"Creating 'submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your 'submission.csv' file is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bec54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v6, Category Fix) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Engineering features...\n",
      "Converting data types and learning categories...\n",
      "Training on 587201 samples, validating on 146801 samples.\n",
      "Applying preprocessing...\n",
      "Preprocessing complete.\n",
      "Defining model to run on CPU...\n",
      "\n",
      "ðŸŽï¸  Starting model training...\n",
      "âœ… Training complete in 72.89 seconds.\n",
      "\n",
      "--- ðŸ Model Evaluation ---\n",
      "Validation RMSE: 11.2226 seconds (Lower is better)\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Applying transformations to test data...\n",
      "Engineering features...\n",
      "Making predictions on test data...\n",
      "Creating 'submission.csv'...\n",
      "\n",
      "âœ… Done! Your 'submission.csv' file is ready to be uploaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v6, Category Fix) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "\n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "\n",
    "# This will store the learned categories from the training data\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Feature Engineering Function ---\n",
    "def engineer_features(df):\n",
    "    \"\"\"Creates new features.\"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # We must operate on a local copy of the feature lists\n",
    "    num_features = NUMERIC_FEATURES.copy()\n",
    "    \n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features:\n",
    "            num_features.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns and 'Ambient_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        \n",
    "        if 'temp_diff' not in num_features:\n",
    "            num_features.append('temp_diff')\n",
    "            \n",
    "    return df, num_features\n",
    "\n",
    "# --- 4. Apply Feature Engineering to Training Data ---\n",
    "df, num_features_list = engineer_features(df_raw.copy())\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce') \n",
    "        \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df.columns:\n",
    "        # Convert to category and store the learned categories\n",
    "        df[col] = df[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df[col].dtype\n",
    "\n",
    "# Safety checks\n",
    "num_features_list = [col for col in num_features_list if col in df.columns]\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df.columns]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    print(f\"FATAL ERROR: Target column '{TARGET}' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df.drop(TARGET, axis=1)\n",
    "y = df[TARGET]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features_list)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    \"\"\"\n",
    "    Applies scaling to numeric data and enforces correct category\n",
    "    dtypes on categorical data.\n",
    "    \"\"\"\n",
    "    # 1. Transform the numeric features\n",
    "    scaled_numeric_data = processor.transform(df[num_feats])\n",
    "    df_numeric_scaled = pd.DataFrame(scaled_numeric_data, columns=num_feats, index=df.index)\n",
    "    \n",
    "    # 2. Get the categorical features and apply *trained* dtypes\n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        # Use pd.Categorical to enforce the learned categories\n",
    "        # Any new, unseen categories in the test set will become NaN\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    \n",
    "    # 3. Concatenate\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    return df_final\n",
    "\n",
    "# Apply to train and validation sets\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model (CPU Fallback) ---\n",
    "print(\"Defining model to run on CPU...\")\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.03,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=50,\n",
    "    tree_method='hist',      # Use the fast CPU method\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# --- 10. Train the Model ---\n",
    "print(\"\\nðŸŽï¸  Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_final, y_val)], \n",
    "    verbose=False \n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Training complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 11. Evaluate Your New Model ---\n",
    "y_pred = model.predict(X_val_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"\\n--- ðŸ Model Evaluation ---\")\n",
    "print(f\"Validation RMSE: {rmse:.4f} seconds (Lower is better)\")\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "try:\n",
    "    df_test_raw = pd.read_csv('test.csv')\n",
    "    submission_ids = df_test_raw['id']\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'test.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "# 1. Apply feature engineering\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy())\n",
    "\n",
    "# 2. Ensure all *numeric* columns are numeric (in case they weren't)\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "# Note: We do *not* convert categorical columns yet\n",
    "\n",
    "# 3. Apply the full preprocessing function\n",
    "# This will scale the numeric columns and *enforce the trained categories*\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,  # Use the *original* list the preprocessor was trained on\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "# This should now work, as the dtypes are identical\n",
    "final_predictions = model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "# Handle any potential NaNs in prediction (if test data had unseen categories)\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your 'submission.csv' file is ready to be uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291de0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: 734002 rows\n",
      "After 'Session' filter: 105815 rows\n",
      "After 'First Lap' filter: 98599 rows\n",
      "After 'Pit Stop' filter: 0 rows\n",
      "After 'Penalty' filter: 0 rows\n",
      "After 'Track Condition' filter: 0 rows\n",
      "Final cleaned data: 0 rows\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# --- 2. Apply CORRECT Filters ---\n",
    "print(f\"Original data: {len(df)} rows\")\n",
    "\n",
    "# --- Filter 1: Session ---\n",
    "# --- YOU MUST EDIT 'Race' ---\n",
    "# Change 'Race' to whatever you found in your investigation\n",
    "RACE_SESSION_NAME = 'Race' \n",
    "df_clean = df[df['Session'] == RACE_SESSION_NAME].copy()\n",
    "print(f\"After 'Session' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# --- Filter 2: First Lap ---\n",
    "# We use 'seq' (current lap), not 'Laps' (total laps)\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "print(f\"After 'First Lap' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# --- Filter 3: Pit Stops ---\n",
    "df_clean = df_clean[df_clean['Pit_Stop_Duration_Seconds'].fillna(0) == 0].copy()\n",
    "print(f\"After 'Pit Stop' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# --- Filter 4: Penalties / DNFs ---\n",
    "# A much better filter: keep all laps *except* DNF/DNS\n",
    "interrupted_laps = ['DNF', 'DNS']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(interrupted_laps)].copy()\n",
    "print(f\"After 'Penalty' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# --- Filter 5: Track Conditions ---\n",
    "# This is probably correct, but check your investigation output\n",
    "valid_conditions = ['Dry', 'Wet']\n",
    "df_clean = df_clean[df_clean['Formula_Track_Condition'].isin(valid_conditions)].copy()\n",
    "print(f\"After 'Track Condition' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# --- Final Check ---\n",
    "print(f\"Final cleaned data: {len(df_clean)} rows\")\n",
    "\n",
    "# Now, use this 'df_clean' for your feature engineering and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13aefb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Investigating Columns for New Filters ---\n",
      "\n",
      "[ Lap_Time_Seconds Stats ]\n",
      "count    734002.000000\n",
      "mean         89.996788\n",
      "std          11.531972\n",
      "min          70.001000\n",
      "25%          79.989000\n",
      "50%          89.970000\n",
      "75%          99.914000\n",
      "90%         106.053000\n",
      "95%         108.028000\n",
      "99%         109.599000\n",
      "max         109.999000\n",
      "Name: Lap_Time_Seconds, dtype: float64\n",
      "--------------------\n",
      "\n",
      "[ Pit_Stop_Duration_Seconds Stats ]\n",
      "count    734002.000000\n",
      "mean          3.502278\n",
      "std           0.868022\n",
      "min           2.000000\n",
      "25%           2.750000\n",
      "50%           3.510000\n",
      "75%           4.250000\n",
      "max           5.000000\n",
      "Name: Pit_Stop_Duration_Seconds, dtype: float64\n",
      "\n",
      "[ Example values for Pit_Stop_Duration ]\n",
      "Pit_Stop_Duration_Seconds\n",
      "2.69    2963\n",
      "3.83    2898\n",
      "3.88    2882\n",
      "2.46    2860\n",
      "3.79    2855\n",
      "4.97    2840\n",
      "3.90    2839\n",
      "3.51    2838\n",
      "2.04    2831\n",
      "2.82    2818\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "\n",
      "[ Penalty Values ]\n",
      "Penalty\n",
      "DNS             123474\n",
      "NaN             122881\n",
      "+3s             122875\n",
      "DNF             122213\n",
      "Ride Through    121345\n",
      "+5s             121214\n",
      "Name: count, dtype: int64\n",
      "--------------------\n",
      "\n",
      "[ Track Condition Values ]\n",
      "Formula_Track_Condition\n",
      "Wet    368110\n",
      "Dry    365892\n",
      "Name: count, dtype: int64\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your full training data\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "print(\"--- Investigating Columns for New Filters ---\")\n",
    "\n",
    "# --- 1. Investigate 'Lap_Time_Seconds' (to find a good cutoff) ---\n",
    "print(\"\\n[ Lap_Time_Seconds Stats ]\")\n",
    "# We'll look at the \"quantiles\" (percentiles)\n",
    "# A 'normal' lap will be near the 50% (median)\n",
    "# A 'pit stop' lap will be up near the 90-95%\n",
    "print(df['Lap_Time_Seconds'].describe(percentiles=[.25, .5, .75, .90, .95, .99]))\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# --- 2. Investigate 'Pit_Stop_Duration_Seconds' (to confirm it's bad) ---\n",
    "print(\"\\n[ Pit_Stop_Duration_Seconds Stats ]\")\n",
    "# This will show us what the values *actually* look like\n",
    "print(df['Pit_Stop_Duration_Seconds'].describe())\n",
    "print(\"\\n[ Example values for Pit_Stop_Duration ]\")\n",
    "print(df['Pit_Stop_Duration_Seconds'].value_counts().head(10))\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# --- 3. Investigate 'Penalty' ---\n",
    "print(\"\\n[ Penalty Values ]\")\n",
    "print(df['Penalty'].value_counts(dropna=False))\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# --- 4. Investigate 'Formula_Track_Condition' ---\n",
    "print(\"\\n[ Track Condition Values ]\")\n",
    "print(df['Formula_Track_Condition'].value_counts(dropna=False))\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v8, Corrected Filters) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "\n",
    "# Base features (we will add more)\n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    # 'Pit_Stop_Duration_Seconds' is now a real feature, not a filter!\n",
    "    'Pit_Stop_Duration_Seconds', \n",
    "    'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "# This will store the learned categories from the training data\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Apply CORRECTED Data Filters ---\n",
    "print(f\"Original data: {len(df_raw)} rows\")\n",
    "\n",
    "# Filter 1: Session\n",
    "df_clean = df_raw[df_raw['Session'] == 'Race'].copy()\n",
    "print(f\"After 'Session' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# Filter 2: First Lap (using 'seq' as current lap)\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "print(f\"After 'First Lap' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# Filter 3: Penalties (Remove non-racing laps)\n",
    "# Keep NaN, +3s, +5s. Remove DNF, DNS, Ride Through.\n",
    "bad_laps = ['DNF', 'DNS', 'Ride Through']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(bad_laps)].copy()\n",
    "print(f\"After 'Penalty' filter: {len(df_clean)} rows\")\n",
    "\n",
    "# NOTE: We no longer filter on 'Pit_Stop_Duration_Seconds'\n",
    "# NOTE: We no longer need to filter on 'Formula_Track_Condition'\n",
    "\n",
    "if len(df_clean) == 0:\n",
    "    print(\"FATAL ERROR: Filtering resulted in 0 rows. Check filters again.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. ADVANCED Feature Engineering ---\n",
    "def engineer_features(df, num_features):\n",
    "    \"\"\"Creates new, high-value features.\"\"\"\n",
    "    print(\"Engineering advanced features...\")\n",
    "    \n",
    "    num_features_copy = num_features.copy()\n",
    "    \n",
    "    # Feature 1: Tire Age\n",
    "    # This is still a guess, but a much better one.\n",
    "    # We create a 'stint_id' by looking for 'Penalty' == 'Ride Through',\n",
    "    # as that's the only other way to know a \"slow\" lap.\n",
    "    # A better way would be to see if Tire_Compound changes.\n",
    "    df['is_ride_through'] = (df['Penalty'] == 'Ride Through').astype(int)\n",
    "    df['stint_id'] = df.groupby('Rider_ID')['is_ride_through'].cumsum()\n",
    "    df['tire_age'] = df.groupby(['Rider_ID', 'stint_id']).cumcount()\n",
    "    if 'tire_age' not in num_features_copy:\n",
    "        num_features_copy.append('tire_age')\n",
    "    print(\"Engineered 'tire_age' feature.\")\n",
    "\n",
    "    # Feature 2: Weather Interactions\n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features_copy:\n",
    "            num_features_copy.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        if 'temp_diff' not in num_features_copy:\n",
    "            num_features_copy.append('temp_diff')\n",
    "    \n",
    "    # Feature 3: Driver/Team Recent Form\n",
    "    df = df.sort_values(by=['race_year', 'seq'])\n",
    "    if 'points' in df.columns:\n",
    "        df['driver_rolling_points'] = df.groupby('Rider_ID')['points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        if 'driver_rolling_points' not in num_features_copy:\n",
    "            num_features_copy.append('driver_rolling_points')\n",
    "        print(\"Engineered 'driver_rolling_points' feature.\")\n",
    "\n",
    "    return df, num_features_copy\n",
    "\n",
    "# Apply FE *only* to the clean data\n",
    "df_featured, num_features_list = engineer_features(df_clean.copy(), NUMERIC_FEATURES)\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce') \n",
    "        \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = df_featured[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df_featured[col].dtype\n",
    "\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df_featured.columns]\n",
    "\n",
    "if TARGET not in df_featured.columns:\n",
    "    print(f\"FATAL ERROR: Target column '{TARGET}' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df_featured.drop(TARGET, axis=1)\n",
    "y = df_featured[TARGET]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_features_list)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    \"\"\"Applies scaling and enforces category dtypes.\"\"\"\n",
    "    df_numeric_scaled = pd.DataFrame(processor.transform(df[num_feats]), columns=num_feats, index=df.index)\n",
    "    \n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    \n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    return df_final\n",
    "\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model (Tuned Parameters) ---\n",
    "print(\"Defining tuned model to run on CPU...\")\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=8,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=100,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# --- 10. Train the Model ---\n",
    "print(\"\\nðŸŽï¸  Starting model training...\")\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_final, y_val)], \n",
    "    verbose=False \n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Training complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 11. Evaluate Your New Model ---\n",
    "y_pred = model.predict(X_val_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"\\n--- ðŸ Model Evaluation ---\")\n",
    "print(f\"New Validation RMSE: {rmse:.4f} seconds (This should be much better)\")\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "try:\n",
    "    df_test_raw = pd.read_csv('test.csv')\n",
    "    submission_ids = df_test_raw['id']\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'test.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "# 1. Apply feature engineering\n",
    "#    NOTE: We do NOT filter the test set.\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy(), NUMERIC_FEATURES)\n",
    "\n",
    "# 2. Ensure all *numeric* columns are numeric\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "\n",
    "# 3. Apply the full preprocessing function\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "# Fill any rows that became NaN (due to unseen categories) with the average lap time\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your new 'submission.csv' is ready. Submit this and check your score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75cca94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v8, Corrected Filters) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Original data: 734002 rows\n",
      "After 'Session' filter: 105815 rows\n",
      "After 'First Lap' filter: 98599 rows\n",
      "After 'Penalty' filter: 49074 rows\n",
      "Engineering advanced features...\n",
      "Converting data types and learning categories...\n",
      "Training on 39259 samples, validating on 9815 samples.\n",
      "Applying preprocessing...\n",
      "Preprocessing complete.\n",
      "Defining tuned model to run on CPU...\n",
      "\n",
      "ðŸŽï¸  Starting model training...\n",
      "âœ… Training complete in 1.88 seconds.\n",
      "\n",
      "--- ðŸ Model Evaluation ---\n",
      "New Validation RMSE: 11.5441 seconds (Write this down!)\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Applying transformations to test data...\n",
      "Engineering advanced features...\n",
      "Making predictions on test data...\n",
      "Creating 'submission.csv'...\n",
      "\n",
      "âœ… Done! Your new 'submission.csv' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v8, Corrected Filters) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Apply CORRECTED Data Filters ---\n",
    "print(f\"Original data: {len(df_raw)} rows\")\n",
    "df_clean = df_raw[df_raw['Session'] == 'Race'].copy()\n",
    "print(f\"After 'Session' filter: {len(df_clean)} rows\")\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "print(f\"After 'First Lap' filter: {len(df_clean)} rows\")\n",
    "bad_laps = ['DNF', 'DNS', 'Ride Through']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(bad_laps)].copy()\n",
    "print(f\"After 'Penalty' filter: {len(df_clean)} rows\")\n",
    "if len(df_clean) == 0:\n",
    "    print(\"FATAL ERROR: Filtering resulted in 0 rows. Check filters again.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. ADVANCED Feature Engineering ---\n",
    "def engineer_features(df, num_features):\n",
    "    print(\"Engineering advanced features...\")\n",
    "    num_features_copy = num_features.copy()\n",
    "    \n",
    "    df['is_ride_through'] = (df['Penalty'] == 'Ride Through').astype(int)\n",
    "    df['stint_id'] = df.groupby('Rider_ID')['is_ride_through'].cumsum()\n",
    "    df['tire_age'] = df.groupby(['Rider_ID', 'stint_id']).cumcount()\n",
    "    if 'tire_age' not in num_features_copy:\n",
    "        num_features_copy.append('tire_age')\n",
    "\n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features_copy:\n",
    "            num_features_copy.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        if 'temp_diff' not in num_features_copy:\n",
    "            num_features_copy.append('temp_diff')\n",
    "    \n",
    "    df = df.sort_values(by=['race_year', 'seq'])\n",
    "    if 'points' in df.columns:\n",
    "        df['driver_rolling_points'] = df.groupby('Rider_ID')['points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        if 'driver_rolling_points' not in num_features_copy:\n",
    "            num_features_copy.append('driver_rolling_points')\n",
    "\n",
    "    return df, num_features_copy\n",
    "\n",
    "df_featured, num_features_list = engineer_features(df_clean.copy(), NUMERIC_FEATURES)\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce') \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = df_featured[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df_featured[col].dtype\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df_featured.columns]\n",
    "if TARGET not in df_featured.columns:\n",
    "    print(\"FATAL ERROR: Target column not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df_featured.drop(TARGET, axis=1)\n",
    "y = df_featured[TARGET]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, num_features_list)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    df_numeric_scaled = pd.DataFrame(processor.transform(df[num_feats]), columns=num_feats, index=df.index)\n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    return df_final\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model (TUNING SECTION) ---\n",
    "print(\"Defining tuned model to run on CPU...\")\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    \n",
    "    # --- KNOB 1: `n_estimators` & `early_stopping_rounds` ---\n",
    "    # We set n_estimators high and let early_stopping find the best number.\n",
    "    # This is already tuned for you!\n",
    "    n_estimators=3000,           # Set this high\n",
    "    early_stopping_rounds=100,   # Stops when the score hasn't improved for 100 trees\n",
    "    \n",
    "    # --- KNOB 2: `learning_rate` ---\n",
    "    # Smaller is often more accurate but slower.\n",
    "    # Try 0.05, then 0.02, then 0.01\n",
    "    learning_rate=0.02,\n",
    "    \n",
    "    # --- KNOB 3: `max_depth` ---\n",
    "    # Deeper trees are more complex. Risk of overfitting.\n",
    "    # Try 6, then 8, then 10.\n",
    "    max_depth=8,\n",
    "    \n",
    "    # --- KNOB 4: Regularization ---\n",
    "    # Helps prevent overfitting. Values from 0.6 to 0.9 are good.\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    \n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# --- 10. Train the Model ---\n",
    "print(\"\\nðŸŽï¸  Starting model training...\")\n",
    "start_time = time.time()\n",
    "model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_val_final, y_val)], \n",
    "    verbose=False \n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Training complete in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 11. Evaluate Your New Model ---\n",
    "y_pred = model.predict(X_val_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "print(f\"\\n--- ðŸ Model Evaluation ---\")\n",
    "print(f\"New Validation RMSE: {rmse:.4f} seconds (Write this down!)\")\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "try:\n",
    "    df_test_raw = pd.read_csv('test.csv')\n",
    "    submission_ids = df_test_raw['id']\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'test.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy(), NUMERIC_FEATURES)\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('submission1.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your new 'submission.csv' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b78f8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.3.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-win_amd64.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 5.5/11.3 MB 25.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.3 MB 26.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 22.2 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.7.2-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 5.0/8.9 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.3/8.9 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 15.4 MB/s eta 0:00:00\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.9/72.0 MB 13.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 5.8/72.0 MB 13.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 8.4/72.0 MB 13.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 11.0/72.0 MB 13.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 14.4/72.0 MB 13.5 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 14.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 23.3/72.0 MB 15.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 25.4/72.0 MB 14.6 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 28.3/72.0 MB 14.4 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 31.5/72.0 MB 14.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 35.1/72.0 MB 14.8 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 39.8/72.0 MB 15.3 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 44.6/72.0 MB 15.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 50.1/72.0 MB 16.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 54.5/72.0 MB 16.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 59.8/72.0 MB 17.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 65.3/72.0 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  70.3/72.0 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 18.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 15.8 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading numpy-2.3.4-cp311-cp311-win_amd64.whl (13.1 MB)\n",
      "   ---------------------------------------- 0.0/13.1 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 5.8/13.1 MB 29.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.5/13.1 MB 27.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.8/13.1 MB 26.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.1/13.1 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.3-cp311-cp311-win_amd64.whl (38.7 MB)\n",
      "   ---------------------------------------- 0.0/38.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.2/38.7 MB 19.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.4/38.7 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 15.7/38.7 MB 24.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.7/38.7 MB 24.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 26.0/38.7 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 31.7/38.7 MB 24.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.7 MB 25.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.7 MB 25.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.7/38.7 MB 20.0 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, xgboost, scikit-learn\n",
      "Successfully installed joblib-1.5.2 numpy-2.3.4 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0 tzdata-2025.2 xgboost-3.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\manna\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db092187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v9, Auto-Tuning) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Original data: 734002 rows\n",
      "After all filters, 49074 rows remain for training.\n",
      "Engineering advanced features (v2)...\n",
      "Engineered improved 'tire_age' feature.\n",
      "Converting data types and learning categories...\n",
      "Total training samples: 39259\n",
      "Using 31407 for tuning, 7852 for validation.\n",
      "Applying preprocessing...\n",
      "Preprocessing complete.\n",
      "Defining model and tuning search space...\n",
      "Starting RandomizedSearchCV... This will be slow!\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "âœ… Tuner finished in 0.62 minutes.\n",
      "Best parameters found: {'colsample_bytree': np.float64(0.7334834444556088), 'learning_rate': np.float64(0.02428668179219408), 'max_depth': 8, 'subsample': np.float64(0.608233797718321)}\n",
      "Best tuning score (RMSE): 11.4910\n",
      "\n",
      "Training the final, best model on ALL training data...\n",
      "\n",
      "--- ðŸ FINAL Model Evaluation ---\n",
      "Final Test Set RMSE: 11.5207 seconds\n",
      "This is the most reliable score for your new model.\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Applying transformations to test data...\n",
      "Engineering advanced features (v2)...\n",
      "Engineered improved 'tire_age' feature.\n",
      "Making predictions on test data...\n",
      "Creating 'final_tuned_submission.csv'...\n",
      "\n",
      "âœ… Done! Your new 'final_tuned_submission.csv' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v9, Auto-Tuning) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Apply CORRECTED Data Filters ---\n",
    "print(f\"Original data: {len(df_raw)} rows\")\n",
    "df_clean = df_raw[df_raw['Session'] == 'Race'].copy()\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "bad_laps = ['DNF', 'DNS', 'Ride Through']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(bad_laps)].copy()\n",
    "print(f\"After all filters, {len(df_clean)} rows remain for training.\")\n",
    "if len(df_clean) == 0:\n",
    "    print(\"FATAL ERROR: Filtering resulted in 0 rows.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. ADVANCED Feature Engineering (v2) ---\n",
    "def engineer_features(df, num_features):\n",
    "    \"\"\"Creates new, high-value features.\"\"\"\n",
    "    print(\"Engineering advanced features (v2)...\")\n",
    "    \n",
    "    num_features_copy = num_features.copy()\n",
    "    \n",
    "    # --- Feature 1: Tire Age (Improved) ---\n",
    "    # A 'stint' is defined by a driver AND a tire compound.\n",
    "    # We sort by lap ('seq') to ensure the order is correct.\n",
    "    df = df.sort_values(by=['race_year', 'Rider_ID', 'seq'])\n",
    "    \n",
    "    # Create a 'stint_id' that changes when the Tire_Compound changes\n",
    "    df['stint_id'] = (df['Tire_Compound'] != df.groupby('Rider_ID')['Tire_Compound'].shift()).astype(int).cumsum()\n",
    "    \n",
    "    # Now, 'tire_age' is the lap count *within that stint*\n",
    "    df['tire_age'] = df.groupby(['Rider_ID', 'stint_id']).cumcount()\n",
    "    if 'tire_age' not in num_features_copy:\n",
    "        num_features_copy.append('tire_age')\n",
    "    print(\"Engineered improved 'tire_age' feature.\")\n",
    "\n",
    "    # --- Feature 2: Weather Interactions ---\n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features_copy:\n",
    "            num_features_copy.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        if 'temp_diff' not in num_features_copy:\n",
    "            num_features_copy.append('temp_diff')\n",
    "\n",
    "        # --- New Interaction Feature ---\n",
    "        df['tire_age_x_track_temp'] = df['tire_age'] * track_temp\n",
    "        if 'tire_age_x_track_temp' not in num_features_copy:\n",
    "            num_features_copy.append('tire_age_x_track_temp')\n",
    "            \n",
    "    # --- Feature 3: Driver/Team Recent Form ---\n",
    "    if 'points' in df.columns:\n",
    "        df['driver_rolling_points'] = df.groupby('Rider_ID')['points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        if 'driver_rolling_points' not in num_features_copy:\n",
    "            num_features_copy.append('driver_rolling_points')\n",
    "\n",
    "    return df, num_features_copy\n",
    "\n",
    "df_featured, num_features_list = engineer_features(df_clean.copy(), NUMERIC_FEATURES)\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce') \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = df_featured[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df_featured[col].dtype\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df_featured.columns]\n",
    "if TARGET not in df_featured.columns:\n",
    "    print(\"FATAL ERROR: Target column not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df_featured.drop(TARGET, axis=1)\n",
    "y = df_featured[TARGET]\n",
    "# We create a final hold-out test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# And a smaller validation set for the tuner\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total training samples: {len(X_train)}\")\n",
    "print(f\"Using {len(X_train_sub)} for tuning, {len(X_val)} for validation.\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, num_features_list)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    df_numeric_scaled = pd.DataFrame(processor.transform(df[num_feats]), columns=num_feats, index=df.index)\n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    return df_final\n",
    "\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_test_final = preprocess_data(X_test, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "\n",
    "# Preprocess the tuning sets\n",
    "X_train_sub_final = preprocess_data(X_train_sub, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model and Tuning Parameters ---\n",
    "print(\"Defining model and tuning search space...\")\n",
    "# Base model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=2000,           # Start high, will use early stopping\n",
    "    \n",
    "    # --- FIX IS HERE ---\n",
    "    # Moved early_stopping_rounds from .fit() to the constructor\n",
    "    early_stopping_rounds=100,\n",
    "    # -------------------\n",
    "    \n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    enable_categorical=True\n",
    ")\n",
    "\n",
    "# Parameter grid for RandomizedSearchCV\n",
    "# This will test random combinations of these settings\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.1),  # Try values between 0.01 and 0.11\n",
    "    'max_depth': randint(6, 12),         # Try depths from 6 to 11\n",
    "    'subsample': uniform(0.6, 0.4),      # Try 0.6 to 1.0\n",
    "    'colsample_bytree': uniform(0.6, 0.4) # Try 0.6 to 1.0\n",
    "}\n",
    "\n",
    "# Set up the Randomized Search\n",
    "print(\"Starting RandomizedSearchCV... This will be slow!\")\n",
    "start_time = time.time()\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Try 10 combinations. Increase to 20 or 30 for better results.\n",
    "    cv=3,       # 3-fold cross-validation\n",
    "    scoring='neg_root_mean_squared_error', # The metric to optimize\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1,  # Show progress\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the search on the *smaller* training subset\n",
    "# --- FIX IS HERE ---\n",
    "# We removed early_stopping_rounds from this call\n",
    "random_search.fit(\n",
    "    X=X_train_sub_final, \n",
    "    y=y_train_sub,\n",
    "    eval_set=[(X_val_final, y_val)], # This eval_set is for the tuner\n",
    "    verbose=False\n",
    ")\n",
    "# -------------------\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Tuner finished in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best tuning score (RMSE): {-random_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 10. Train the FINAL Model ---\n",
    "print(\"\\nTraining the final, best model on ALL training data...\")\n",
    "# Get the best model from the search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Now, retrain it on the *entire* training set\n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# We remove 'early_stopping_rounds' because the 'best_model'\n",
    "# already has this setting from when it was created in Section 9.\n",
    "best_model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_final, y_test)], # Use the hold-out test set for stopping\n",
    "    verbose=False\n",
    ")\n",
    "# -------------------\n",
    "\n",
    "# --- 11. Evaluate Your New Model on the Test Set ---\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"\\n--- ðŸ FINAL Model Evaluation ---\")\n",
    "print(f\"Final Test Set RMSE: {rmse:.4f} seconds\")\n",
    "print(\"This is the most reliable score for your new model.\")\n",
    "\n",
    "# ... (The rest of your script, from Section 12 onwards, is fine) ...\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "df_test_raw = pd.read_csv('test.csv')\n",
    "submission_ids = df_test_raw['id']\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy(), NUMERIC_FEATURES)\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = best_model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'final_tuned_submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('final_tuned_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your new 'final_tuned_submission.csv' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ece371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v12, KeyError Fix) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Original data: 734002 rows\n",
      "After all filters, 49074 rows remain for training.\n",
      "Engineering advanced features (v3.1)...\n",
      "Engineered 'previous_lap_time' feature.\n",
      "Converting data types and learning categories...\n",
      "Total training samples: 2552, Total test samples: 639\n",
      "Applying preprocessing...\n",
      "Preprocessing complete.\n",
      "Defining model and tuning search space...\n",
      "Starting RandomizedSearchCV... This will be slow!\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "âœ… Tuner finished in 0.32 minutes.\n",
      "Best parameters found: {'colsample_bytree': np.float64(0.7243929286862649), 'learning_rate': np.float64(0.026259166101337356), 'max_depth': 8, 'subsample': np.float64(0.8365191150830908)}\n",
      "Best tuning score (RMSE): 11.5446\n",
      "\n",
      "Training the final, best model on ALL training data...\n",
      "\n",
      "--- ðŸ FINAL Model Evaluation ---\n",
      "Final Test Set RMSE: 11.8274 seconds\n",
      "This is the most reliable score for your new model.\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Applying transformations to test data...\n",
      "Engineering advanced features (v3.1)...\n",
      "Created 'previous_lap_time' placeholder for test set.\n",
      "Making predictions on test data...\n",
      "Creating 'v12_tuned_submission.csv'...\n",
      "\n",
      "âœ… Done! Your new 'v12_tuned_submission.csv' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v12, KeyError Fix) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Apply CORRECTED Data Filters ---\n",
    "print(f\"Original data: {len(df_raw)} rows\")\n",
    "df_clean = df_raw[df_raw['Session'] == 'Race'].copy()\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "bad_laps = ['DNF', 'DNS', 'Ride Through']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(bad_laps)].copy()\n",
    "print(f\"After all filters, {len(df_clean)} rows remain for training.\")\n",
    "if len(df_clean) == 0:\n",
    "    print(\"FATAL ERROR: Filtering resulted in 0 rows.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. ADVANCED Feature Engineering (v3.1 - With Lag Fix) ---\n",
    "def engineer_features(df, num_features, is_train=True):\n",
    "    \"\"\"Creates new, high-value features.\"\"\"\n",
    "    print(\"Engineering advanced features (v3.1)...\")\n",
    "    \n",
    "    num_features_copy = num_features.copy()\n",
    "    \n",
    "    # Sort by time to create time-based features\n",
    "    df = df.sort_values(by=['race_year', 'Rider_ID', 'seq'])\n",
    "    \n",
    "    # Feature 1: Tire Age (Improved)\n",
    "    df['stint_id'] = (df['Tire_Compound'] != df.groupby('Rider_ID')['Tire_Compound'].shift()).astype(int).cumsum()\n",
    "    df['tire_age'] = df.groupby(['Rider_ID', 'stint_id']).cumcount()\n",
    "    if 'tire_age' not in num_features_copy:\n",
    "        num_features_copy.append('tire_age')\n",
    "\n",
    "    # Feature 2: Weather Interactions\n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features_copy:\n",
    "            num_features_copy.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        if 'temp_diff' not in num_features_copy:\n",
    "            num_features_copy.append('temp_diff')\n",
    "\n",
    "        df['tire_age_x_track_temp'] = df['tire_age'] * track_temp\n",
    "        if 'tire_age_x_track_temp' not in num_features_copy:\n",
    "            num_features_copy.append('tire_age_x_track_temp')\n",
    "            \n",
    "    # Feature 3: Driver/Team Recent Form\n",
    "    if 'points' in df.columns:\n",
    "        df['driver_rolling_points'] = df.groupby('Rider_ID')['points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        if 'driver_rolling_points' not in num_features_copy:\n",
    "            num_features_copy.append('driver_rolling_points')\n",
    "            \n",
    "    # --- Feature 4: Lag Feature (FIXED) ---\n",
    "    if is_train:\n",
    "        # If training, create it from the target\n",
    "        df['previous_lap_time'] = df.groupby(['Rider_ID', 'stint_id'])[TARGET].shift(1)\n",
    "        print(\"Engineered 'previous_lap_time' feature.\")\n",
    "    else:\n",
    "        # If testing, create a placeholder NaN column\n",
    "        df['previous_lap_time'] = np.nan\n",
    "        print(\"Created 'previous_lap_time' placeholder for test set.\")\n",
    "    \n",
    "    # Add the feature to the list *outside* the if/else\n",
    "    if 'previous_lap_time' not in num_features_copy:\n",
    "        num_features_copy.append('previous_lap_time')\n",
    "    # ---------------------------------\n",
    "    \n",
    "    # Drop rows that have NaN from rolling/shift features (only for training)\n",
    "    if is_train:\n",
    "        # We now have two features that create NaNs on the first few rows\n",
    "        df = df.dropna(subset=['driver_rolling_points', 'previous_lap_time'])\n",
    "\n",
    "    return df, num_features_copy\n",
    "\n",
    "df_featured, num_features_list = engineer_features(df_clean.copy(), NUMERIC_FEATURES, is_train=True)\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce') \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = df_featured[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df_featured[col].dtype\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df_featured.columns]\n",
    "if TARGET not in df_featured.columns:\n",
    "    print(\"FATAL ERROR: Target column not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df_featured.drop(TARGET, axis=1)\n",
    "y = df_featured[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')), # Median is robust to outliers\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, num_features_list)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    df_numeric_scaled = pd.DataFrame(processor.transform(df[num_feats]), columns=num_feats, index=df.index)\n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    df_final[cat_feats] = df_final[cat_feats].apply(lambda x: x.cat.codes)\n",
    "    return df_final\n",
    "\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_test_final = preprocess_data(X_test, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_train_sub_final = preprocess_data(X_train_sub, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model and Tuning Parameters ---\n",
    "print(\"Defining model and tuning search space...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=3000,\n",
    "    early_stopping_rounds=100,\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.05),\n",
    "    'max_depth': randint(6, 14),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "print(\"Starting RandomizedSearchCV... This will be slow!\")\n",
    "start_time = time.time()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    cv=3,       \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  \n",
    "    verbose=1,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(\n",
    "    X=X_train_sub_final, \n",
    "    y=y_train_sub,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Tuner finished in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best tuning score (RMSE): {-random_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 10. Train the FINAL Model ---\n",
    "print(\"\\nTraining the final, best model on ALL training data...\")\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "best_model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_final, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- 11. Evaluate Your New Model on the Test Set ---\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"\\n--- ðŸ FINAL Model Evaluation ---\")\n",
    "print(f\"Final Test Set RMSE: {rmse:.4f} seconds\")\n",
    "print(\"This is the most reliable score for your new model.\")\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "df_test_raw = pd.read_csv('test.csv')\n",
    "submission_ids = df_test_raw['id']\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "# We now use is_train=False\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy(), NUMERIC_FEATURES, is_train=False)\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "\n",
    "# This call will now work\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = best_model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'v12_tuned_submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('v12_tuned_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your new 'v12_tuned_submission.csv' is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1636c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting F1 Lap Time Model Training (v14, FAST-TUNE) ---\n",
      "Loading 'train.csv'...\n",
      "Loaded 'train.csv' with 734002 rows.\n",
      "Original data: 734002 rows\n",
      "After all filters, 49074 rows remain for training.\n",
      "Engineering advanced features (v3.1)...\n",
      "Converting data types and learning categories...\n",
      "Total training samples: 2552, Total test samples: 639\n",
      "Applying preprocessing...\n",
      "Preprocessing complete.\n",
      "Defining model and tuning search space...\n",
      "Starting RandomizedSearchCV (n_iter=10)...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "âœ… Tuner finished in 0.11 minutes.\n",
      "Best parameters found: {'colsample_bytree': np.float64(0.7727780074568463), 'learning_rate': np.float64(0.024561457009902095), 'max_depth': 8, 'subsample': np.float64(0.7599443886861021)}\n",
      "Best tuning score (RMSE): 11.5574\n",
      "\n",
      "Training the final, best model on ALL training data...\n",
      "\n",
      "--- ðŸ FINAL Model Evaluation ---\n",
      "Final Test Set RMSE: 11.8145 seconds\n",
      "This is the most reliable score for your new model.\n",
      "\n",
      "Loading 'test.csv' for final predictions...\n",
      "Applying transformations to test data...\n",
      "Engineering advanced features (v3.1)...\n",
      "Making predictions on test data...\n",
      "Creating 'v14_fast_submission.csv'...\n",
      "\n",
      "âœ… Done! Your new 'v14_fast_submission.csv' is ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import sys\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "print(f\"--- Starting F1 Lap Time Model Training (v14, FAST-TUNE) ---\")\n",
    "\n",
    "# --- 1. Define Features & Target ---\n",
    "TARGET = 'Lap_Time_Seconds' \n",
    "NUMERIC_FEATURES = [\n",
    "    'Len_Circuit_inkm', 'Laps', 'Start_Position', 'Formula_Avg_Speed_kmh',\n",
    "    'Humidity_%', 'Champ_Points', 'Champ_Position', 'race_year',\n",
    "    'seq', 'position', 'points', 'Corners_in_Lap', 'Tire_Degradation_Factor_per_Lap',\n",
    "    'Pit_Stop_Duration_Seconds', 'Ambient_Temperature_Celsius', \n",
    "    'Track_Temperature_Celsius', 'air', 'ground', 'starts', 'finishes',\n",
    "    'with_points', 'podiums', 'wins'\n",
    "]\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'Rider_ID', 'Formula_category_x', 'Formula_Track_Condition',\n",
    "    'Tire_Compound', 'Penalty', 'Session', 'Formula_shortname',\n",
    "    'circuit_name', 'weather', 'track'\n",
    "]\n",
    "CATEGORY_DTYPES = {}\n",
    "\n",
    "# --- 2. Load Your Training Data ---\n",
    "try:\n",
    "    print(\"Loading 'train.csv'...\")\n",
    "    df_raw = pd.read_csv('train.csv')\n",
    "    print(f\"Loaded 'train.csv' with {df_raw.shape[0]} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: 'train.csv' not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 3. Apply CORRECTED Data Filters ---\n",
    "print(f\"Original data: {len(df_raw)} rows\")\n",
    "df_clean = df_raw[df_raw['Session'] == 'Race'].copy()\n",
    "df_clean = df_clean[df_clean['seq'] > 1].copy()\n",
    "bad_laps = ['DNF', 'DNS', 'Ride Through']\n",
    "df_clean = df_clean[~df_clean['Penalty'].isin(bad_laps)].copy()\n",
    "print(f\"After all filters, {len(df_clean)} rows remain for training.\")\n",
    "if len(df_clean) == 0:\n",
    "    print(\"FATAL ERROR: Filtering resulted in 0 rows.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 4. ADVANCED Feature Engineering (v3.1 - With Lag Fix) ---\n",
    "def engineer_features(df, num_features, is_train=True):\n",
    "    \"\"\"Creates new, high-value features.\"\"\"\n",
    "    print(\"Engineering advanced features (v3.1)...\")\n",
    "    \n",
    "    num_features_copy = num_features.copy()\n",
    "    \n",
    "    df = df.sort_values(by=['race_year', 'Rider_ID', 'seq'])\n",
    "    \n",
    "    # Tire Age\n",
    "    df['stint_id'] = (df['Tire_Compound'] != df.groupby('Rider_ID')['Tire_Compound'].shift()).astype(int).cumsum()\n",
    "    df['tire_age'] = df.groupby(['Rider_ID', 'stint_id']).cumcount()\n",
    "    if 'tire_age' not in num_features_copy:\n",
    "        num_features_copy.append('tire_age')\n",
    "\n",
    "    # Weather Interactions\n",
    "    if 'weather' in df.columns:\n",
    "        df['is_raining'] = df['weather'].astype(str).str.contains('Rain', case=False).fillna(0)\n",
    "        if 'is_raining' not in num_features_copy:\n",
    "            num_features_copy.append('is_raining')\n",
    "\n",
    "    if 'Track_Temperature_Celsius' in df.columns:\n",
    "        track_temp = pd.to_numeric(df['Track_Temperature_Celsius'], errors='coerce')\n",
    "        ambient_temp = pd.to_numeric(df['Ambient_Temperature_Celsius'], errors='coerce')\n",
    "        df['temp_diff'] = track_temp - ambient_temp\n",
    "        if 'temp_diff' not in num_features_copy:\n",
    "            num_features_copy.append('temp_diff')\n",
    "\n",
    "        df['tire_age_x_track_temp'] = df['tire_age'] * track_temp\n",
    "        if 'tire_age_x_track_temp' not in num_features_copy:\n",
    "            num_features_copy.append('tire_age_x_track_temp')\n",
    "            \n",
    "    # Driver Form\n",
    "    if 'points' in df.columns:\n",
    "        df['driver_rolling_points'] = df.groupby('Rider_ID')['points'].rolling(window=5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "        if 'driver_rolling_points' not in num_features_copy:\n",
    "            num_features_copy.append('driver_rolling_points')\n",
    "            \n",
    "    # Lag Feature\n",
    "    if is_train:\n",
    "        df['previous_lap_time'] = df.groupby(['Rider_ID', 'stint_id'])[TARGET].shift(1)\n",
    "    else:\n",
    "        df['previous_lap_time'] = np.nan\n",
    "    if 'previous_lap_time' not in num_features_copy:\n",
    "        num_features_copy.append('previous_lap_time')\n",
    "    \n",
    "    if is_train:\n",
    "        df = df.dropna(subset=['driver_rolling_points', 'previous_lap_time'])\n",
    "\n",
    "    return df, num_features_copy\n",
    "\n",
    "df_featured, num_features_list = engineer_features(df_clean.copy(), NUMERIC_FEATURES, is_train=True)\n",
    "\n",
    "# --- 5. Data Type Conversion (Training Data) ---\n",
    "print(\"Converting data types and learning categories...\")\n",
    "for col in num_features_list:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = pd.to_numeric(df_featured[col], errors='coerce') \n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_featured.columns:\n",
    "        df_featured[col] = df_featured[col].astype('category')\n",
    "        CATEGORY_DTYPES[col] = df_featured[col].dtype\n",
    "cat_features_list = [col for col in CATEGORICAL_FEATURES if col in df_featured.columns]\n",
    "if TARGET not in df_featured.columns:\n",
    "    print(\"FATAL ERROR: Target column not found.\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- 6. Create Data Splits ---\n",
    "X = df_featured.drop(TARGET, axis=1)\n",
    "y = df_featured[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(f\"Total training samples: {len(X_train)}, Total test samples: {len(X_test)}\")\n",
    "\n",
    "# --- 7. Set up Preprocessing ---\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', numeric_transformer, num_features_list)],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# --- 8. Apply Preprocessing Manually ---\n",
    "print(\"Applying preprocessing...\")\n",
    "preprocessor.fit(X_train[num_features_list])\n",
    "def preprocess_data(df, processor, num_feats, cat_feats, cat_dtypes):\n",
    "    df_numeric_scaled = pd.DataFrame(processor.transform(df[num_feats]), columns=num_feats, index=df.index)\n",
    "    df_categorical = pd.DataFrame(index=df.index)\n",
    "    for col in cat_feats:\n",
    "        df_categorical[col] = pd.Categorical(df[col], categories=cat_dtypes[col].categories)\n",
    "    df_final = pd.concat([df_numeric_scaled, df_categorical], axis=1)\n",
    "    df_final[cat_feats] = df_final[cat_feats].apply(lambda x: x.cat.codes)\n",
    "    return df_final\n",
    "\n",
    "X_train_final = preprocess_data(X_train, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_test_final = preprocess_data(X_test, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_train_sub_final = preprocess_data(X_train_sub, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "X_val_final = preprocess_data(X_val, preprocessor, num_features_list, cat_features_list, CATEGORY_DTYPES)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "# --- 9. Define the XGBoost Model and Tuning Parameters ---\n",
    "print(\"Defining model and tuning search space...\")\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=3000,\n",
    "    early_stopping_rounds=100,\n",
    "    random_state=42,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(0.01, 0.05),\n",
    "    'max_depth': randint(6, 12),  # Slightly simpler depth range\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4)\n",
    "}\n",
    "\n",
    "print(\"Starting RandomizedSearchCV (n_iter=10)...\")\n",
    "start_time = time.time()\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # <-- FAST TUNING (was 30)\n",
    "    cv=3,       \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,  \n",
    "    verbose=1,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(\n",
    "    X=X_train_sub_final, \n",
    "    y=y_train_sub,\n",
    "    eval_set=[(X_val_final, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"âœ… Tuner finished in {(end_time - start_time) / 60:.2f} minutes.\")\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best tuning score (RMSE): {-random_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 10. Train the FINAL Model ---\n",
    "print(\"\\nTraining the final, best model on ALL training data...\")\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "best_model.fit(\n",
    "    X_train_final, \n",
    "    y_train,\n",
    "    eval_set=[(X_test_final, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- 11. Evaluate Your New Model on the Test Set ---\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"\\n--- ðŸ FINAL Model Evaluation ---\")\n",
    "print(f\"Final Test Set RMSE: {rmse:.4f} seconds\")\n",
    "print(\"This is the most reliable score for your new model.\")\n",
    "\n",
    "# --- 12. Load 'test.csv' for Final Predictions ---\n",
    "print(\"\\nLoading 'test.csv' for final predictions...\")\n",
    "df_test_raw = pd.read_csv('test.csv')\n",
    "submission_ids = df_test_raw['id']\n",
    "\n",
    "# --- 13. Apply the SAME Transformations to Test Data ---\n",
    "print(\"Applying transformations to test data...\")\n",
    "df_test_engineered, test_num_feats = engineer_features(df_test_raw.copy(), NUMERIC_FEATURES, is_train=False)\n",
    "for col in test_num_feats:\n",
    "    if col in df_test_engineered.columns:\n",
    "        df_test_engineered[col] = pd.to_numeric(df_test_engineered[col], errors='coerce')\n",
    "\n",
    "df_test_final = preprocess_data(\n",
    "    df_test_engineered, \n",
    "    preprocessor, \n",
    "    num_features_list,\n",
    "    cat_features_list, \n",
    "    CATEGORY_DTYPES\n",
    ")\n",
    "\n",
    "# --- 14. Make Final Predictions ---\n",
    "print(\"Making predictions on test data...\")\n",
    "final_predictions = best_model.predict(df_test_final)\n",
    "\n",
    "# --- 15. Create Submission File ---\n",
    "print(\"Creating 'v14_fast_submission.csv'...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': submission_ids,\n",
    "    'Lap_Time_Seconds': final_predictions\n",
    "})\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].fillna(y_train.mean())\n",
    "submission_df['Lap_Time_Seconds'] = submission_df['Lap_Time_Seconds'].apply(lambda x: max(0, x))\n",
    "submission_df.to_csv('v14_fast_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Done! Your new 'v14_fast_submission.csv' is ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
